# Comment-Toxicity-Project
The project employs a Bidirectional LSTM model to classify online comments as toxic, obscene, or hateful. It preprocesses text using TensorFlowâ€™s TextVectorization, trains on the Jigsaw Toxic Comment dataset, and evaluates performance using accuracy, precision, and recall, achieving high validation accuracy.
